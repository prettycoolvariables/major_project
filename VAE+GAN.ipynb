{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (recall_score, f1_score,confusion_matrix, roc_auc_score, average_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(directory, sr=22050, duration=5):\n",
    "    #Loads audio files from a directory\n",
    "    audio_data = []\n",
    "    for folder in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue        \n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            if not file_path.endswith(('.wav', '.mp3')):\n",
    "                continue            \n",
    "            try:\n",
    "                signal, _ = librosa.load(file_path, sr=sr, duration=duration)\n",
    "                audio_data.append(signal)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "    return np.array(audio_data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio, sr=22050):\n",
    "    #Apply simple pitch shifting and noise injection to augment audio\n",
    "    n_steps = np.random.uniform(-2, 2)\n",
    "    audio_shifted = librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=n_steps)\n",
    "    noise = 0.005 * np.random.randn(len(audio_shifted))\n",
    "    return audio_shifted + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectrograms(audio_data, sr=22050, n_fft=2048, hop_length=512, n_mels=128, max_time_frames=128):\n",
    "    #Computes mel-spectrograms (in decibels) and normalizes them to [0,1].\n",
    "    specs = []\n",
    "    for sample in audio_data:\n",
    "        if len(sample) == 0:\n",
    "            continue\n",
    "        mel_spec = librosa.feature.melspectrogram(y=sample, sr=sr, n_fft=n_fft,\n",
    "                                                  hop_length=hop_length, n_mels=n_mels)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        # Add epsilon to denominator to avoid division by zero.\n",
    "        mel_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-9)\n",
    "        if mel_norm.shape[1] < max_time_frames:\n",
    "            pad_width = max_time_frames - mel_norm.shape[1]\n",
    "            mel_norm = np.pad(mel_norm, ((0,0),(0,pad_width)), mode='constant')\n",
    "        else:\n",
    "            mel_norm = mel_norm[:, :max_time_frames]\n",
    "        specs.append(mel_norm)\n",
    "    return np.array(specs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_spectrogram_images(specs):\n",
    "    #Expands spectrograms to have a channel dimension.\n",
    "    return np.expand_dims(specs, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_encoder(latent_dim, input_shape=(128,128,1)):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same', activation='relu')(inputs)  # 64x64x32\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding='same', activation='relu')(x)       # 32x32x64\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding='same', activation='relu')(x)      # 16x16x128\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    return keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "def build_decoder(latent_dim, output_shape=(128,128,1)):\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(16*16*128, activation='relu')(latent_inputs)\n",
    "    x = layers.Reshape((16,16,128))(x)\n",
    "    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)  # 32x32x128\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(x)   # 64x64x64\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu')(x)   # 128x128x32\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Conv2DTranspose(1, 3, padding='same', activation='sigmoid')(x)       # 128x128x1\n",
    "    return keras.Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "def build_discriminator(input_shape=(128,128,1)):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return keras.Model(inputs, outputs, name='discriminator')\n",
    "\n",
    "class VAE_GAN(keras.Model):\n",
    "    def __init__(self, encoder, decoder, discriminator, lambda_adv=0.5, **kwargs):\n",
    "        super(VAE_GAN, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.discriminator = discriminator\n",
    "        self.lambda_adv = lambda_adv\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs, training=training)\n",
    "        reconstructed = self.decoder(z, training=training)\n",
    "        return reconstructed\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape_gen, tf.GradientTape() as tape_disc:\n",
    "            z_mean, z_log_var, z = self.encoder(data, training=True)\n",
    "            # Clip z_log_var to avoid numerical issues in exp\n",
    "            z_log_var = tf.clip_by_value(z_log_var, -10.0, 10.0)\n",
    "            reconstructed = self.decoder(z, training=True)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.keras.losses.mse(data, reconstructed))\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "           \n",
    "            disc_pred_fake = self.discriminator(reconstructed, training=True)\n",
    "            # Clip discriminator predictions to avoid log(0) issues in binary crossentropy\n",
    "            disc_pred_fake = tf.clip_by_value(disc_pred_fake, 1e-7, 1.0 - 1e-7)\n",
    "            valid_labels = tf.ones_like(disc_pred_fake)\n",
    "            adv_loss = tf.keras.losses.binary_crossentropy(valid_labels, disc_pred_fake)\n",
    "            adv_loss = tf.reduce_mean(adv_loss)\n",
    "            gen_loss = reconstruction_loss + kl_loss + self.lambda_adv * adv_loss\n",
    "\n",
    "            disc_pred_real = self.discriminator(data, training=True)\n",
    "            disc_pred_real = tf.clip_by_value(disc_pred_real, 1e-7, 1.0 - 1e-7)\n",
    "            real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(disc_pred_real), disc_pred_real)\n",
    "            fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(disc_pred_fake), disc_pred_fake)\n",
    "            disc_loss = tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss)\n",
    "        # Compute gradients\n",
    "        grads_gen = tape_gen.gradient(gen_loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n",
    "        grads_disc = tape_disc.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        # Apply global gradient clipping to prevent explosion\n",
    "        grads_gen, _ = tf.clip_by_global_norm(grads_gen, 1.0)\n",
    "        grads_disc, _ = tf.clip_by_global_norm(grads_disc, 1.0)\n",
    "        # Apply gradients\n",
    "        self.gen_optimizer.apply_gradients(zip(grads_gen, self.encoder.trainable_variables + self.decoder.trainable_variables))\n",
    "        self.disc_optimizer.apply_gradients(zip(grads_disc, self.discriminator.trainable_variables))\n",
    "        return {\"gen_loss\": gen_loss,\n",
    "                \"reconstruction_loss\": reconstruction_loss,\n",
    "                \"kl_loss\": kl_loss,\n",
    "                \"adv_loss\": adv_loss,\n",
    "                \"disc_loss\": disc_loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        z_mean, z_log_var, z = self.encoder(data, training=False)\n",
    "        z_log_var = tf.clip_by_value(z_log_var, -10.0, 10.0)\n",
    "        reconstructed = self.decoder(z, training=False)\n",
    "        reconstruction_loss = tf.reduce_mean(tf.keras.losses.mse(data, reconstructed))\n",
    "        kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        disc_pred_fake = self.discriminator(reconstructed, training=False)\n",
    "        disc_pred_fake = tf.clip_by_value(disc_pred_fake, 1e-7, 1.0 - 1e-7)\n",
    "        valid_labels = tf.ones_like(disc_pred_fake)\n",
    "        adv_loss = tf.keras.losses.binary_crossentropy(valid_labels, disc_pred_fake)\n",
    "        adv_loss = tf.reduce_mean(adv_loss)\n",
    "        gen_loss = reconstruction_loss + kl_loss + self.lambda_adv * adv_loss\n",
    "        return {\"gen_loss\": gen_loss,\n",
    "                \"reconstruction_loss\": reconstruction_loss,\n",
    "                \"kl_loss\": kl_loss,\n",
    "                \"adv_loss\": adv_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_anomalies(images, noise_factor=2.0):\n",
    "    #Create synthetic anomalies by adding heavy Gaussian noise.\n",
    "    anomalies = images + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)\n",
    "    return np.clip(anomalies, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_threshold(losses, k=1.5):\n",
    "    return np.mean(losses) + k * np.std(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reconstruction_loss(original, reconstructed):\n",
    "    return np.mean(np.square(original - reconstructed), axis=(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths \n",
    "dataset_path = \"/Users/aiswaryamariamjacob/Major Project/audio\"\n",
    "test_path = \"/Users/aiswaryamariamjacob/Major Project/Raw Audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X_train shape: (28008, 128, 128, 1)\n",
      "Final X_val shape: (7002, 128, 128, 1)\n",
      "Final X_test shape: (1677, 128, 128, 1)\n",
      "Epoch 1/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1898s\u001b[0m 2s/step - adv_loss: 13.4682 - disc_loss: 0.1806 - gen_loss: 7.1337 - kl_loss: 0.2923 - reconstruction_loss: 0.1073 - val_adv_loss: 14.3531 - val_gen_loss: 8.5190 - val_kl_loss: 1.2636 - val_reconstruction_loss: 0.0788\n",
      "Epoch 2/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1874s\u001b[0m 2s/step - adv_loss: 7.3353 - disc_loss: 1.0845 - gen_loss: 3.9497 - kl_loss: 0.2428 - reconstruction_loss: 0.0393 - val_adv_loss: 7.7084 - val_gen_loss: 3.8854 - val_kl_loss: 0.0010 - val_reconstruction_loss: 0.0301\n",
      "Epoch 3/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1858s\u001b[0m 2s/step - adv_loss: 7.8641 - disc_loss: 0.5654 - gen_loss: 3.9686 - kl_loss: 0.0017 - reconstruction_loss: 0.0349 - val_adv_loss: 13.6546 - val_gen_loss: 6.8586 - val_kl_loss: 0.0016 - val_reconstruction_loss: 0.0297\n",
      "Epoch 4/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1867s\u001b[0m 2s/step - adv_loss: 8.0142 - disc_loss: 0.4825 - gen_loss: 4.0457 - kl_loss: 0.0035 - reconstruction_loss: 0.0351 - val_adv_loss: 6.7024 - val_gen_loss: 3.3853 - val_kl_loss: 0.0021 - val_reconstruction_loss: 0.0320\n",
      "Epoch 5/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1855s\u001b[0m 2s/step - adv_loss: 9.6933 - disc_loss: 0.3117 - gen_loss: 4.8831 - kl_loss: 0.0039 - reconstruction_loss: 0.0326 - val_adv_loss: 14.9009 - val_gen_loss: 7.4748 - val_kl_loss: 0.0025 - val_reconstruction_loss: 0.0219\n",
      "Epoch 6/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1861s\u001b[0m 2s/step - adv_loss: 9.5948 - disc_loss: 0.2483 - gen_loss: 4.8330 - kl_loss: 0.0030 - reconstruction_loss: 0.0326 - val_adv_loss: 10.7984 - val_gen_loss: 5.4256 - val_kl_loss: 0.0026 - val_reconstruction_loss: 0.0238\n",
      "Epoch 7/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1860s\u001b[0m 2s/step - adv_loss: 11.1279 - disc_loss: 0.3710 - gen_loss: 5.5986 - kl_loss: 0.0040 - reconstruction_loss: 0.0307 - val_adv_loss: 11.4027 - val_gen_loss: 5.7325 - val_kl_loss: 0.0031 - val_reconstruction_loss: 0.0281\n",
      "Epoch 8/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1859s\u001b[0m 2s/step - adv_loss: 12.0468 - disc_loss: 0.2406 - gen_loss: 6.0582 - kl_loss: 0.0036 - reconstruction_loss: 0.0312 - val_adv_loss: 14.7915 - val_gen_loss: 7.4278 - val_kl_loss: 0.0031 - val_reconstruction_loss: 0.0289\n",
      "Epoch 9/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1861s\u001b[0m 2s/step - adv_loss: 10.7044 - disc_loss: 0.6241 - gen_loss: 5.3860 - kl_loss: 0.0034 - reconstruction_loss: 0.0303 - val_adv_loss: 15.3015 - val_gen_loss: 7.6836 - val_kl_loss: 0.0035 - val_reconstruction_loss: 0.0294\n",
      "Epoch 10/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1861s\u001b[0m 2s/step - adv_loss: 10.0711 - disc_loss: 0.9837 - gen_loss: 5.0720 - kl_loss: 0.0036 - reconstruction_loss: 0.0328 - val_adv_loss: 13.4667 - val_gen_loss: 6.7711 - val_kl_loss: 0.0039 - val_reconstruction_loss: 0.0339\n",
      "Epoch 11/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1862s\u001b[0m 2s/step - adv_loss: 9.7791 - disc_loss: 0.8809 - gen_loss: 4.9225 - kl_loss: 0.0040 - reconstruction_loss: 0.0289 - val_adv_loss: 2.6422 - val_gen_loss: 1.3470 - val_kl_loss: 0.0043 - val_reconstruction_loss: 0.0216\n",
      "Epoch 12/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1882s\u001b[0m 2s/step - adv_loss: 10.1757 - disc_loss: 0.6618 - gen_loss: 5.1239 - kl_loss: 0.0042 - reconstruction_loss: 0.0318 - val_adv_loss: 14.1100 - val_gen_loss: 7.0868 - val_kl_loss: 0.0044 - val_reconstruction_loss: 0.0275\n",
      "Epoch 13/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1860s\u001b[0m 2s/step - adv_loss: 9.1512 - disc_loss: 0.6131 - gen_loss: 4.6101 - kl_loss: 0.0045 - reconstruction_loss: 0.0300 - val_adv_loss: 7.5110 - val_gen_loss: 3.7846 - val_kl_loss: 0.0046 - val_reconstruction_loss: 0.0246\n",
      "Epoch 14/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1858s\u001b[0m 2s/step - adv_loss: 8.6163 - disc_loss: 0.8428 - gen_loss: 4.3428 - kl_loss: 0.0045 - reconstruction_loss: 0.0302 - val_adv_loss: 10.4278 - val_gen_loss: 5.2478 - val_kl_loss: 0.0046 - val_reconstruction_loss: 0.0293\n",
      "Epoch 15/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1861s\u001b[0m 2s/step - adv_loss: 9.5150 - disc_loss: 0.5092 - gen_loss: 4.7932 - kl_loss: 0.0048 - reconstruction_loss: 0.0309 - val_adv_loss: 13.9604 - val_gen_loss: 7.0134 - val_kl_loss: 0.0050 - val_reconstruction_loss: 0.0281\n",
      "Epoch 16/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1861s\u001b[0m 2s/step - adv_loss: 8.6779 - disc_loss: 0.7429 - gen_loss: 4.3742 - kl_loss: 0.0053 - reconstruction_loss: 0.0299 - val_adv_loss: 13.3763 - val_gen_loss: 6.7163 - val_kl_loss: 0.0058 - val_reconstruction_loss: 0.0224\n",
      "Epoch 17/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1867s\u001b[0m 2s/step - adv_loss: 8.6366 - disc_loss: 0.8378 - gen_loss: 4.3542 - kl_loss: 0.0057 - reconstruction_loss: 0.0302 - val_adv_loss: 5.8354 - val_gen_loss: 2.9566 - val_kl_loss: 0.0058 - val_reconstruction_loss: 0.0331\n",
      "Epoch 18/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1866s\u001b[0m 2s/step - adv_loss: 8.3304 - disc_loss: 0.8538 - gen_loss: 4.2006 - kl_loss: 0.0059 - reconstruction_loss: 0.0295 - val_adv_loss: 11.8234 - val_gen_loss: 5.9478 - val_kl_loss: 0.0061 - val_reconstruction_loss: 0.0300\n",
      "Epoch 19/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1872s\u001b[0m 2s/step - adv_loss: 7.6791 - disc_loss: 0.7720 - gen_loss: 3.8758 - kl_loss: 0.0066 - reconstruction_loss: 0.0296 - val_adv_loss: 16.0196 - val_gen_loss: 8.0438 - val_kl_loss: 0.0070 - val_reconstruction_loss: 0.0270\n",
      "Epoch 20/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1870s\u001b[0m 2s/step - adv_loss: 7.8691 - disc_loss: 1.0079 - gen_loss: 3.9723 - kl_loss: 0.0071 - reconstruction_loss: 0.0307 - val_adv_loss: 13.2723 - val_gen_loss: 6.6711 - val_kl_loss: 0.0067 - val_reconstruction_loss: 0.0283\n",
      "Epoch 21/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1873s\u001b[0m 2s/step - adv_loss: 7.2472 - disc_loss: 0.8607 - gen_loss: 3.6594 - kl_loss: 0.0070 - reconstruction_loss: 0.0288 - val_adv_loss: 12.0091 - val_gen_loss: 6.0414 - val_kl_loss: 0.0073 - val_reconstruction_loss: 0.0296\n",
      "Epoch 22/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1874s\u001b[0m 2s/step - adv_loss: 7.2044 - disc_loss: 0.7244 - gen_loss: 3.6384 - kl_loss: 0.0071 - reconstruction_loss: 0.0292 - val_adv_loss: 3.1808 - val_gen_loss: 1.6298 - val_kl_loss: 0.0070 - val_reconstruction_loss: 0.0324\n",
      "Epoch 23/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1872s\u001b[0m 2s/step - adv_loss: 6.2644 - disc_loss: 1.0165 - gen_loss: 3.1696 - kl_loss: 0.0073 - reconstruction_loss: 0.0301 - val_adv_loss: 5.2623 - val_gen_loss: 2.6635 - val_kl_loss: 0.0075 - val_reconstruction_loss: 0.0249\n",
      "Epoch 24/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1894s\u001b[0m 2s/step - adv_loss: 6.3313 - disc_loss: 1.0793 - gen_loss: 3.2025 - kl_loss: 0.0071 - reconstruction_loss: 0.0297 - val_adv_loss: 9.3733 - val_gen_loss: 4.7185 - val_kl_loss: 0.0069 - val_reconstruction_loss: 0.0250\n",
      "Epoch 25/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1916s\u001b[0m 2s/step - adv_loss: 5.8219 - disc_loss: 0.8530 - gen_loss: 2.9481 - kl_loss: 0.0070 - reconstruction_loss: 0.0302 - val_adv_loss: 10.5284 - val_gen_loss: 5.3000 - val_kl_loss: 0.0074 - val_reconstruction_loss: 0.0284\n",
      "Epoch 26/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1900s\u001b[0m 2s/step - adv_loss: 5.9309 - disc_loss: 0.9287 - gen_loss: 3.0035 - kl_loss: 0.0076 - reconstruction_loss: 0.0305 - val_adv_loss: 13.9559 - val_gen_loss: 7.0129 - val_kl_loss: 0.0076 - val_reconstruction_loss: 0.0274\n",
      "Epoch 27/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1881s\u001b[0m 2s/step - adv_loss: 5.6494 - disc_loss: 0.7707 - gen_loss: 2.8625 - kl_loss: 0.0076 - reconstruction_loss: 0.0302 - val_adv_loss: 8.1607 - val_gen_loss: 4.1120 - val_kl_loss: 0.0077 - val_reconstruction_loss: 0.0240\n",
      "Epoch 28/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1894s\u001b[0m 2s/step - adv_loss: 5.4127 - disc_loss: 1.0882 - gen_loss: 2.7437 - kl_loss: 0.0073 - reconstruction_loss: 0.0300 - val_adv_loss: 2.2063 - val_gen_loss: 1.1398 - val_kl_loss: 0.0075 - val_reconstruction_loss: 0.0291\n",
      "Epoch 29/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1896s\u001b[0m 2s/step - adv_loss: 5.5066 - disc_loss: 0.9734 - gen_loss: 2.7910 - kl_loss: 0.0077 - reconstruction_loss: 0.0299 - val_adv_loss: 5.7208 - val_gen_loss: 2.8977 - val_kl_loss: 0.0076 - val_reconstruction_loss: 0.0297\n",
      "Epoch 30/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1869s\u001b[0m 2s/step - adv_loss: 5.3249 - disc_loss: 0.8425 - gen_loss: 2.7004 - kl_loss: 0.0077 - reconstruction_loss: 0.0302 - val_adv_loss: 13.2355 - val_gen_loss: 6.6509 - val_kl_loss: 0.0081 - val_reconstruction_loss: 0.0250\n",
      "Epoch 31/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1866s\u001b[0m 2s/step - adv_loss: 5.0223 - disc_loss: 0.8158 - gen_loss: 2.5484 - kl_loss: 0.0079 - reconstruction_loss: 0.0294 - val_adv_loss: 0.5706 - val_gen_loss: 0.3245 - val_kl_loss: 0.0078 - val_reconstruction_loss: 0.0313\n",
      "Epoch 32/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1873s\u001b[0m 2s/step - adv_loss: 5.1766 - disc_loss: 0.8674 - gen_loss: 2.6256 - kl_loss: 0.0079 - reconstruction_loss: 0.0294 - val_adv_loss: 4.5943 - val_gen_loss: 2.3250 - val_kl_loss: 0.0079 - val_reconstruction_loss: 0.0200\n",
      "Epoch 33/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1869s\u001b[0m 2s/step - adv_loss: 4.4200 - disc_loss: 0.8420 - gen_loss: 2.2481 - kl_loss: 0.0078 - reconstruction_loss: 0.0303 - val_adv_loss: 5.5238 - val_gen_loss: 2.8008 - val_kl_loss: 0.0081 - val_reconstruction_loss: 0.0308\n",
      "Epoch 34/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1865s\u001b[0m 2s/step - adv_loss: 4.7958 - disc_loss: 0.9628 - gen_loss: 2.4351 - kl_loss: 0.0080 - reconstruction_loss: 0.0291 - val_adv_loss: 11.1425 - val_gen_loss: 5.6023 - val_kl_loss: 0.0084 - val_reconstruction_loss: 0.0226\n",
      "Epoch 35/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1859s\u001b[0m 2s/step - adv_loss: 5.1142 - disc_loss: 1.0436 - gen_loss: 2.5953 - kl_loss: 0.0086 - reconstruction_loss: 0.0295 - val_adv_loss: 14.4805 - val_gen_loss: 7.2761 - val_kl_loss: 0.0086 - val_reconstruction_loss: 0.0273\n",
      "Epoch 36/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1863s\u001b[0m 2s/step - adv_loss: 4.9556 - disc_loss: 1.1005 - gen_loss: 2.5161 - kl_loss: 0.0086 - reconstruction_loss: 0.0297 - val_adv_loss: 12.7389 - val_gen_loss: 6.4054 - val_kl_loss: 0.0086 - val_reconstruction_loss: 0.0273\n",
      "Epoch 37/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1867s\u001b[0m 2s/step - adv_loss: 4.8540 - disc_loss: 1.0140 - gen_loss: 2.4651 - kl_loss: 0.0089 - reconstruction_loss: 0.0292 - val_adv_loss: 5.4592 - val_gen_loss: 2.7703 - val_kl_loss: 0.0090 - val_reconstruction_loss: 0.0317\n",
      "Epoch 38/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1869s\u001b[0m 2s/step - adv_loss: 4.7648 - disc_loss: 1.0598 - gen_loss: 2.4207 - kl_loss: 0.0088 - reconstruction_loss: 0.0295 - val_adv_loss: 14.9947 - val_gen_loss: 7.5243 - val_kl_loss: 0.0088 - val_reconstruction_loss: 0.0181\n",
      "Epoch 39/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1865s\u001b[0m 2s/step - adv_loss: 4.4218 - disc_loss: 1.1201 - gen_loss: 2.2488 - kl_loss: 0.0088 - reconstruction_loss: 0.0291 - val_adv_loss: 14.0043 - val_gen_loss: 7.0361 - val_kl_loss: 0.0088 - val_reconstruction_loss: 0.0251\n",
      "Epoch 40/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1868s\u001b[0m 2s/step - adv_loss: 4.8266 - disc_loss: 0.8553 - gen_loss: 2.4524 - kl_loss: 0.0089 - reconstruction_loss: 0.0302 - val_adv_loss: 14.9496 - val_gen_loss: 7.5140 - val_kl_loss: 0.0089 - val_reconstruction_loss: 0.0304\n",
      "Epoch 41/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1869s\u001b[0m 2s/step - adv_loss: 4.6027 - disc_loss: 0.8107 - gen_loss: 2.3393 - kl_loss: 0.0090 - reconstruction_loss: 0.0290 - val_adv_loss: 2.2205 - val_gen_loss: 1.1423 - val_kl_loss: 0.0089 - val_reconstruction_loss: 0.0231\n",
      "Epoch 42/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1861s\u001b[0m 2s/step - adv_loss: 4.3241 - disc_loss: 1.0130 - gen_loss: 2.2006 - kl_loss: 0.0094 - reconstruction_loss: 0.0291 - val_adv_loss: 4.7971 - val_gen_loss: 2.4376 - val_kl_loss: 0.0095 - val_reconstruction_loss: 0.0296\n",
      "Epoch 43/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1865s\u001b[0m 2s/step - adv_loss: 3.9732 - disc_loss: 0.9945 - gen_loss: 2.0244 - kl_loss: 0.0093 - reconstruction_loss: 0.0285 - val_adv_loss: 13.4454 - val_gen_loss: 6.7547 - val_kl_loss: 0.0094 - val_reconstruction_loss: 0.0227\n",
      "Epoch 44/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1865s\u001b[0m 2s/step - adv_loss: 4.0189 - disc_loss: 0.9175 - gen_loss: 2.0480 - kl_loss: 0.0095 - reconstruction_loss: 0.0290 - val_adv_loss: 9.0384 - val_gen_loss: 4.5504 - val_kl_loss: 0.0098 - val_reconstruction_loss: 0.0214\n",
      "Epoch 45/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1865s\u001b[0m 2s/step - adv_loss: 4.3946 - disc_loss: 1.0794 - gen_loss: 2.2355 - kl_loss: 0.0100 - reconstruction_loss: 0.0282 - val_adv_loss: 1.8760 - val_gen_loss: 0.9785 - val_kl_loss: 0.0102 - val_reconstruction_loss: 0.0303\n",
      "Epoch 46/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1868s\u001b[0m 2s/step - adv_loss: 4.3871 - disc_loss: 1.0401 - gen_loss: 2.2340 - kl_loss: 0.0104 - reconstruction_loss: 0.0301 - val_adv_loss: 5.6868 - val_gen_loss: 2.8841 - val_kl_loss: 0.0109 - val_reconstruction_loss: 0.0298\n",
      "Epoch 47/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1914s\u001b[0m 2s/step - adv_loss: 4.7306 - disc_loss: 0.9245 - gen_loss: 2.4062 - kl_loss: 0.0107 - reconstruction_loss: 0.0302 - val_adv_loss: 4.4143 - val_gen_loss: 2.2407 - val_kl_loss: 0.0106 - val_reconstruction_loss: 0.0229\n",
      "Epoch 48/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1923s\u001b[0m 2s/step - adv_loss: 4.0766 - disc_loss: 1.0489 - gen_loss: 2.0798 - kl_loss: 0.0109 - reconstruction_loss: 0.0307 - val_adv_loss: 8.5967 - val_gen_loss: 4.3367 - val_kl_loss: 0.0106 - val_reconstruction_loss: 0.0277\n",
      "Epoch 49/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1907s\u001b[0m 2s/step - adv_loss: 4.4944 - disc_loss: 0.8403 - gen_loss: 2.2876 - kl_loss: 0.0109 - reconstruction_loss: 0.0295 - val_adv_loss: 2.3775 - val_gen_loss: 1.2240 - val_kl_loss: 0.0108 - val_reconstruction_loss: 0.0244\n",
      "Epoch 50/50\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1903s\u001b[0m 2s/step - adv_loss: 4.0811 - disc_loss: 0.9046 - gen_loss: 2.0805 - kl_loss: 0.0109 - reconstruction_loss: 0.0290 - val_adv_loss: 6.1015 - val_gen_loss: 3.0879 - val_kl_loss: 0.0110 - val_reconstruction_loss: 0.0262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14a47ed10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load audio files\n",
    "audio_samples = load_audio_files(dataset_path)\n",
    "test_samples = load_audio_files(test_path)\n",
    "\n",
    "# 2. Augment audio for training\n",
    "augmented_audio = [augment_audio(a) for a in audio_samples]\n",
    "audio_train_all = list(audio_samples) + augmented_audio\n",
    "\n",
    "# 3. Extract mel-spectrograms (in decibels) from audio\n",
    "specs_train = extract_spectrograms(audio_train_all, sr=22050, n_fft=2048, hop_length=512, n_mels=128, max_time_frames=128)\n",
    "specs_test = extract_spectrograms(test_samples, sr=22050, n_fft=2048, hop_length=512, n_mels=128, max_time_frames=128)\n",
    "\n",
    "# 4. Expand dims to have channel dimension\n",
    "X_all = prepare_spectrogram_images(specs_train)\n",
    "X_test = prepare_spectrogram_images(specs_test)\n",
    "\n",
    "# 5. Train-Val split (unsupervised; all assumed normal)\n",
    "X_train, X_val = train_test_split(X_all, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Final X_train shape:\", X_train.shape)\n",
    "print(\"Final X_val shape:\", X_val.shape)\n",
    "print(\"Final X_test shape:\", X_test.shape)\n",
    "\n",
    "# 6. Build VAE + GAN components (input shape: (128,128,1), latent_dim=32)\n",
    "latent_dim = 32\n",
    "encoder = build_encoder(latent_dim, input_shape=(128,128,1))\n",
    "decoder = build_decoder(latent_dim, output_shape=(128,128,1))\n",
    "discriminator = build_discriminator(input_shape=(128,128,1))\n",
    "\n",
    "# 7. Initialize VAE + GAN model\n",
    "vae_gan = VAE_GAN(encoder, decoder, discriminator, lambda_adv=0.5)\n",
    "\n",
    "# 8. Set up optimizers with a lower learning rate\n",
    "vae_gan.gen_optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "vae_gan.disc_optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# 8.5 Compile the model (dummy compile; custom train_step is used)\n",
    "vae_gan.compile(optimizer=vae_gan.gen_optimizer)\n",
    "\n",
    "# 9. Train the VAE + GAN\n",
    "vae_gan.fit(X_train, epochs=50, batch_size=32, validation_data=(X_val,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 265ms/step\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 266ms/step\n",
      "Validation adaptive threshold: 0.0596\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 266ms/step\n",
      "Validation Metrics:\n",
      "Recall: 1.0000, F1: 0.9619\n",
      "ROC-AUC: 1.0000, PR-AUC: 1.0000\n",
      "Confusion Matrix:\n",
      "[[6448  554]\n",
      " [   0 7002]]\n"
     ]
    }
   ],
   "source": [
    "# Validation evaluation:\n",
    "val_anomalies = generate_synthetic_anomalies(X_val, noise_factor=2.0)\n",
    "val_reconstructed = vae_gan.predict(X_val, batch_size=32)\n",
    "val_loss_normal = compute_reconstruction_loss(X_val, val_reconstructed)\n",
    "val_reconstructed_anom = vae_gan.predict(val_anomalies, batch_size=32)\n",
    "val_loss_anom = compute_reconstruction_loss(val_anomalies, val_reconstructed_anom)\n",
    "val_threshold = compute_threshold(val_loss_normal, k=1.5)\n",
    "print(f\"Validation adaptive threshold: {val_threshold:.4f}\")\n",
    "X_val_bal = np.concatenate([X_val, val_anomalies], axis=0)\n",
    "y_val_true = np.concatenate([np.zeros(len(X_val)), np.ones(len(val_anomalies))])\n",
    "val_reconstructed_bal = vae_gan.predict(X_val_bal, batch_size=32)\n",
    "val_loss_bal = compute_reconstruction_loss(X_val_bal, val_reconstructed_bal)\n",
    "y_val_pred = (val_loss_bal > val_threshold).astype(int)\n",
    "\n",
    "val_recall = recall_score(y_val_true, y_val_pred, zero_division=1)\n",
    "val_f1 = f1_score(y_val_true, y_val_pred, zero_division=1)\n",
    "val_cm = confusion_matrix(y_val_true, y_val_pred)\n",
    "val_roc = roc_auc_score(y_val_true, val_loss_bal)\n",
    "val_pr  = average_precision_score(y_val_true, val_loss_bal)\n",
    "\n",
    "print(\"Validation Metrics:\")\n",
    "print(f\"Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "print(f\"ROC-AUC: {val_roc:.4f}, PR-AUC: {val_pr:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(val_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 267ms/step\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 263ms/step\n",
      "Test threshold (from validation): 0.0596\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 266ms/step\n",
      "Test Metrics:\n",
      "Recall: 1.0000, F1: 0.6802\n",
      "ROC-AUC: 0.9981, PR-AUC: 0.9939\n",
      "Confusion Matrix:\n",
      "[[ 100 1577]\n",
      " [   0 1677]]\n"
     ]
    }
   ],
   "source": [
    "# Test evaluation:\n",
    "test_anomalies = generate_synthetic_anomalies(X_test, noise_factor=2.0)\n",
    "test_reconstructed = vae_gan.predict(X_test, batch_size=32)\n",
    "test_loss_normal = compute_reconstruction_loss(X_test, test_reconstructed)\n",
    "test_reconstructed_anom = vae_gan.predict(test_anomalies, batch_size=32)\n",
    "test_loss_anom = compute_reconstruction_loss(test_anomalies, test_reconstructed_anom)\n",
    "test_threshold = val_threshold  # using the same threshold from validation\n",
    "print(f\"Test threshold (from validation): {test_threshold:.4f}\")\n",
    "X_test_bal = np.concatenate([X_test, test_anomalies], axis=0)\n",
    "y_test_true = np.concatenate([np.zeros(len(X_test)), np.ones(len(test_anomalies))])\n",
    "test_reconstructed_bal = vae_gan.predict(X_test_bal, batch_size=32)\n",
    "test_loss_bal = compute_reconstruction_loss(X_test_bal, test_reconstructed_bal)\n",
    "y_test_pred = (test_loss_bal > test_threshold).astype(int)\n",
    "\n",
    "test_recall = recall_score(y_test_true, y_test_pred, zero_division=1)\n",
    "test_f1 = f1_score(y_test_true, y_test_pred, zero_division=1)\n",
    "test_cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "test_roc = roc_auc_score(y_test_true, test_loss_bal)\n",
    "test_pr  = average_precision_score(y_test_true, test_loss_bal)\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(f\"Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "print(f\"ROC-AUC: {test_roc:.4f}, PR-AUC: {test_pr:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(test_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_vae_gan/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_vae_gan/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(vae_gan,\"saved_vae_gan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
